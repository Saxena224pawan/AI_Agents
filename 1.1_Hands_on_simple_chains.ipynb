{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Load the YAML file\n",
    "def load_api_key(yml_file):\n",
    "    with open(yml_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)  # Safely load the YAML file\n",
    "        return config.get('openai_key')  # Retrieve the 'api_key'\n",
    "\n",
    "# Usage\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] =load_api_key('chatgpt_api_credentials.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are five important points summarizing the Stable Diffusion architecture:\n",
       "\n",
       "1. **Latent Diffusion Model (LDM)**: Stable Diffusion employs a Latent Diffusion Model, which compresses images into a latent space before applying the diffusion process. This allows for efficient training and generation of high-resolution images while significantly reducing computational requirements.\n",
       "\n",
       "2. **Denoising Process**: The architecture relies on a denoising autoencoder framework, where noise is progressively added to the images during training. The model learns to reverse this process, generating coherent images from random noise through iterative denoising steps.\n",
       "\n",
       "3. **Text-to-Image Synthesis**: Stable Diffusion incorporates a text encoder (often based on CLIP) to align text prompts with visual features, enabling the generation of images that correspond closely to user-provided textual descriptions. This makes it particularly effective for creative and artistic applications.\n",
       "\n",
       "4. **Flexible Conditioning**: The architecture supports various conditioning methods, allowing for the generation of images based on different inputs, including text, images, or even sketches, enhancing its versatility for diverse tasks in image generation.\n",
       "\n",
       "5. **Open-Source and Community-Driven**: Stable Diffusion is open-source, encouraging collaboration and innovation within the research community. Its accessibility has led to widespread adoption and the development of numerous applications and models built on its foundational technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','You are a research assistant'),\n",
    "    ('user','{input}')\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "basic_chain = prompt |  llm | output_parser\n",
    "\n",
    "output = basic_chain.invoke('Write 5 bullet points summarizing important points of Stable diffusion architecture.')\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let;s write a draft of a research report using chains in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Understanding Transformers: A Guide for Non-AI Researchers\n",
       "\n",
       "## Introduction\n",
       "Transformers have revolutionized the field of artificial intelligence (AI), particularly in natural language processing (NLP) and computer vision. This report aims to explain the fundamental concepts of Transformers in a straightforward manner, making it accessible for those without a technical background in AI.\n",
       "\n",
       "## What is a Transformer?\n",
       "A Transformer is a type of neural network architecture that was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. Unlike previous models that processed data sequentially, Transformers can process entire sequences of data simultaneously, making them highly efficient for tasks involving large amounts of information.\n",
       "\n",
       "### Key Components of Transformers\n",
       "1. **Input Representation**: \n",
       "   - Transformers take input data (like words in a sentence) and convert them into numerical vectors. This process is known as embedding, where each word is represented as a point in a high-dimensional space.\n",
       "\n",
       "2. **Attention Mechanism**: \n",
       "   - The core innovation of Transformers is the attention mechanism. This allows the model to weigh the importance of different words in a sentence when making predictions. For example, in the sentence \"The cat sat on the mat,\" the model can focus more on \"cat\" and \"sat\" when predicting the next word.\n",
       "\n",
       "3. **Self-Attention**: \n",
       "   - Self-attention is a specific type of attention where the model looks at all the words in a sentence to determine their relationships. This helps the model understand context better, as it can see how words influence each other.\n",
       "\n",
       "4. **Multi-Head Attention**: \n",
       "   - Instead of having a single attention mechanism, Transformers use multiple attention heads. Each head learns to focus on different parts of the input, allowing the model to capture various relationships and meanings simultaneously.\n",
       "\n",
       "5. **Feed-Forward Neural Networks**: \n",
       "   - After the attention mechanism, the data is passed through feed-forward neural networks, which apply additional transformations to the data. This helps in refining the information before it moves to the next layer.\n",
       "\n",
       "6. **Positional Encoding**: \n",
       "   - Since Transformers process data in parallel, they need a way to understand the order of words. Positional encoding adds information about the position of each word in the sequence, allowing the model to maintain the structure of the input.\n",
       "\n",
       "7. **Stacking Layers**: \n",
       "   - Transformers consist of multiple layers of attention and feed-forward networks stacked on top of each other. This deep architecture enables the model to learn complex patterns in the data.\n",
       "\n",
       "## How Transformers Work\n",
       "1. **Input Processing**: \n",
       "   - The input text is tokenized (broken down into smaller pieces, like words or subwords) and converted into embeddings.\n",
       "\n",
       "2. **Attention Calculation**: \n",
       "   - For each word, the model calculates how much attention it should pay to every other word in the input. This is done using three vectors: Query, Key, and Value. The attention scores are computed by comparing the Query of one word with the Keys of all other words.\n",
       "\n",
       "3. **Aggregation**: \n",
       "   - The attention scores are used to create a weighted sum of the Value vectors, resulting in a new representation of the input that emphasizes important words.\n",
       "\n",
       "4. **Feed-Forward Processing**: \n",
       "   - The aggregated information is passed through feed-forward neural networks, which apply transformations to enhance the representation further.\n",
       "\n",
       "5. **Output Generation**: \n",
       "   - Finally, the processed information is used to generate predictions, such as the next word in a sentence or the classification of an image.\n",
       "\n",
       "## Applications of Transformers\n",
       "Transformers have been successfully applied in various domains, including:\n",
       "- **Natural Language Processing**: Tasks like translation, summarization, and sentiment analysis.\n",
       "- **Computer Vision**: Image classification and object detection.\n",
       "- **Speech Recognition**: Converting spoken language into text.\n",
       "- **Reinforcement Learning**: Enhancing decision-making processes in complex environments.\n",
       "\n",
       "## Conclusion\n",
       "Transformers represent a significant advancement in AI, enabling machines to understand and generate human-like text and process visual information more effectively. By leveraging the attention mechanism and deep learning techniques, Transformers have set new benchmarks in various applications. Understanding the basic principles of Transformers can provide valuable insights into how modern AI systems operate, even for those without a technical background in the field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WRITER_SYS_MSG = \"\"\"\n",
    "You are a research assistant and a scientific writer.\n",
    "YOu take in requests about topics and write an organized research report on those topics\n",
    "\"\"\"\n",
    "prompt =  ChatPromptTemplate.from_messages([\n",
    "    ('system', WRITER_SYS_MSG ),\n",
    "    ('human','Write an organized research report about this topic:\\n\\n{input}.')\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "writer_chain = prompt |  llm | output_parser\n",
    "\n",
    "output = writer_chain.invoke({'input':'How do Transformers work for non AI researchers?.'})\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Research Report: Understanding Stable Diffusion for Non-AI Researchers\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Stable diffusion is a concept that has gained significant attention in the field of artificial intelligence (AI), particularly in the context of generative models. It refers to a class of algorithms that can generate high-quality images, text, and other forms of data by learning from existing datasets. This report aims to explain the principles of stable diffusion in a manner accessible to non-AI researchers, including relevant formulas and concepts.\n",
       "\n",
       "## What is Diffusion?\n",
       "\n",
       "Diffusion, in a general sense, refers to the process by which particles spread from areas of high concentration to areas of low concentration. In the context of AI, diffusion processes are used to model how information or features propagate through a system. \n",
       "\n",
       "### Mathematical Representation of Diffusion\n",
       "\n",
       "The diffusion process can be mathematically represented using partial differential equations (PDEs). One common form is the heat equation:\n",
       "\n",
       "\\[\n",
       "\\frac{\\partial u(x,t)}{\\partial t} = \\Delta u(x,t)\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( u(x,t) \\) is the temperature (or concentration) at position \\( x \\) and time \\( t \\).\n",
       "- \\( \\Delta \\) is the Laplace operator, which represents the diffusion of heat.\n",
       "\n",
       "## Stable Diffusion in AI\n",
       "\n",
       "Stable diffusion models are a type of generative model that utilize diffusion processes to create new data samples. The key idea is to start with a simple distribution (like Gaussian noise) and gradually transform it into a more complex distribution (like an image) through a series of steps.\n",
       "\n",
       "### The Diffusion Process\n",
       "\n",
       "1. **Forward Process**: This involves adding noise to the data over a series of time steps. The forward process can be described mathematically as:\n",
       "\n",
       "\\[\n",
       "q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( x_t \\) is the noisy data at time step \\( t \\).\n",
       "- \\( \\beta_t \\) is a variance schedule that controls the amount of noise added at each step.\n",
       "- \\( \\mathcal{N} \\) denotes a Gaussian distribution.\n",
       "\n",
       "2. **Reverse Process**: The goal is to learn how to reverse the noise addition process, effectively denoising the data. This is modeled as:\n",
       "\n",
       "\\[\n",
       "p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( \\mu_\\theta \\) and \\( \\Sigma_\\theta \\) are learned parameters that define the mean and covariance of the distribution at each step, parameterized by \\( \\theta \\).\n",
       "\n",
       "### Training the Model\n",
       "\n",
       "The model is trained to minimize the difference between the true data distribution and the generated distribution. This is often done using a loss function such as the variational lower bound:\n",
       "\n",
       "\\[\n",
       "L = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( \\epsilon \\) is the noise added to the data.\n",
       "- \\( \\epsilon_\\theta \\) is the model's prediction of the noise.\n",
       "\n",
       "## Applications of Stable Diffusion\n",
       "\n",
       "Stable diffusion models have a wide range of applications, including:\n",
       "\n",
       "- **Image Generation**: Creating realistic images from random noise.\n",
       "- **Text-to-Image Synthesis**: Generating images based on textual descriptions.\n",
       "- **Inpainting**: Filling in missing parts of images.\n",
       "- **Super Resolution**: Enhancing the resolution of images.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Stable diffusion models represent a powerful approach in the field of generative AI, leveraging the principles of diffusion processes to create high-quality data. By understanding the forward and reverse processes, as well as the underlying mathematical formulations, non-AI researchers can appreciate the sophistication and potential of these models. As the field continues to evolve, stable diffusion is likely to play a crucial role in various applications across different domains.\n",
       "\n",
       "## References\n",
       "\n",
       "1. Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. *arXiv preprint arXiv:2006.11239*.\n",
       "2. Song, Y., & Ermon, S. (2019). Generative Modeling by Estimating Gradients of the Data Distribution. *arXiv preprint arXiv:1907.05600*.\n",
       "3. Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *arXiv preprint arXiv:2105.05233*."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = writer_chain.invoke({'input':'How do Stable diffusion work for non AI researchers?. Write with formulas?'})\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Feedback on Research Report: Understanding Stable Diffusion for Non-AI Researchers\n",
       "\n",
       "1. **Clarity and Accessibility**: The report does a commendable job of breaking down complex concepts into more digestible parts for non-AI researchers. However, it could benefit from more intuitive explanations or analogies, especially in the introduction and the sections on the forward and reverse processes. For instance, using a real-world analogy for diffusion could help readers grasp the concept more easily.\n",
       "\n",
       "2. **Visual Aids**: Incorporating diagrams or flowcharts to illustrate the diffusion process, both forward and reverse, would enhance understanding. Visual representations can often clarify complex mathematical concepts and processes, making them more relatable for readers unfamiliar with the subject.\n",
       "\n",
       "3. **Contextual Examples**: While the applications of stable diffusion are listed, providing specific examples or case studies for each application would strengthen the report. For instance, mentioning a particular project or tool that utilizes stable diffusion for image generation could provide practical context and relevance.\n",
       "\n",
       "4. **Mathematical Depth**: The mathematical representations are well-presented, but some readers may find them overwhelming. Consider adding a brief explanation of each formula's significance in layman's terms, or providing a simplified version of the equations to cater to those without a strong mathematical background.\n",
       "\n",
       "5. **Future Directions**: The conclusion could be expanded to include potential future developments in stable diffusion models and their implications for various fields. Discussing ongoing research or emerging trends could engage readers and highlight the dynamic nature of the field, encouraging further exploration.\n",
       "\n",
       "Overall, the report is informative and well-structured, but enhancing clarity, visual engagement, and contextual relevance could significantly improve its effectiveness for non-AI researchers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WRITER_SYS_MSG = \"\"\"\n",
    "You are review\n",
    "\"\"\"\n",
    "prompt_reviewer =  ChatPromptTemplate.from_messages([\n",
    "    ('system', WRITER_SYS_MSG ),\n",
    "    ('human','Provide Feedback on this research report:\\n\\n{input}. Add 5 concise points')\n",
    "])\n",
    "\n",
    "llm_reviewer = ChatOpenAI(model='gpt-4o-mini', temperature=0.24)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "review_chain = prompt_reviewer |  llm_reviewer | output_parser\n",
    "\n",
    "feedback_output = review_chain.invoke({'input':output})\n",
    "\n",
    "Markdown(feedback_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Reviewed Research Report: Understanding Stable Diffusion for Non-AI Researchers\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Stable diffusion is a concept that has gained significant attention in the field of artificial intelligence (AI), particularly in the context of generative models. It refers to a class of algorithms that can generate high-quality images, text, and other forms of data by learning from existing datasets. This report aims to explain the principles of stable diffusion in a manner accessible to non-AI researchers, incorporating intuitive explanations, visual aids, and contextual examples to enhance understanding.\n",
       "\n",
       "## What is Diffusion?\n",
       "\n",
       "In a general sense, diffusion refers to the process by which particles spread from areas of high concentration to areas of low concentration. Imagine a drop of ink in water; over time, the ink spreads out evenly throughout the water. In the context of AI, diffusion processes are used to model how information or features propagate through a system, akin to how the ink disperses in water.\n",
       "\n",
       "### Mathematical Representation of Diffusion\n",
       "\n",
       "The diffusion process can be mathematically represented using partial differential equations (PDEs). One common form is the heat equation:\n",
       "\n",
       "\\[\n",
       "\\frac{\\partial u(x,t)}{\\partial t} = \\Delta u(x,t)\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( u(x,t) \\) represents the temperature (or concentration) at position \\( x \\) and time \\( t \\).\n",
       "- \\( \\Delta \\) is the Laplace operator, which mathematically describes how heat diffuses through a medium.\n",
       "\n",
       "## Stable Diffusion in AI\n",
       "\n",
       "Stable diffusion models are a type of generative model that utilize diffusion processes to create new data samples. The key idea is to start with a simple distribution (like Gaussian noise) and gradually transform it into a more complex distribution (like an image) through a series of steps.\n",
       "\n",
       "### The Diffusion Process\n",
       "\n",
       "To better understand the diffusion process, consider it as a two-step journey: first, we add noise to our data, and then we learn how to remove that noise.\n",
       "\n",
       "1. **Forward Process**: This involves adding noise to the data over a series of time steps. The forward process can be described mathematically as:\n",
       "\n",
       "\\[\n",
       "q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( x_t \\) is the noisy data at time step \\( t \\).\n",
       "- \\( \\beta_t \\) is a variance schedule that controls the amount of noise added at each step.\n",
       "- \\( \\mathcal{N} \\) denotes a Gaussian distribution.\n",
       "\n",
       "*Visual Aid*: A flowchart illustrating the forward process could depict how data becomes increasingly noisy over time.\n",
       "\n",
       "2. **Reverse Process**: The goal is to learn how to reverse the noise addition process, effectively denoising the data. This is modeled as:\n",
       "\n",
       "\\[\n",
       "p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( \\mu_\\theta \\) and \\( \\Sigma_\\theta \\) are learned parameters that define the mean and covariance of the distribution at each step, parameterized by \\( \\theta \\).\n",
       "\n",
       "*Intuitive Explanation*: Think of the reverse process as a skilled artist who starts with a chaotic canvas (noisy data) and gradually refines it into a beautiful painting (denoised data).\n",
       "\n",
       "### Training the Model\n",
       "\n",
       "The model is trained to minimize the difference between the true data distribution and the generated distribution. This is often done using a loss function such as the variational lower bound:\n",
       "\n",
       "\\[\n",
       "L = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\n",
       "\\]\n",
       "\n",
       "Where:\n",
       "- \\( \\epsilon \\) is the noise added to the data.\n",
       "- \\( \\epsilon_\\theta \\) is the model's prediction of the noise.\n",
       "\n",
       "*Simplified Explanation*: The model learns by comparing the noise it predicts with the actual noise added, adjusting its parameters to improve accuracy.\n",
       "\n",
       "## Applications of Stable Diffusion\n",
       "\n",
       "Stable diffusion models have a wide range of applications, including:\n",
       "\n",
       "- **Image Generation**: Creating realistic images from random noise. For example, tools like DALL-E use stable diffusion techniques to generate images based on textual prompts.\n",
       "- **Text-to-Image Synthesis**: Generating images based on textual descriptions, enabling creative applications in art and design.\n",
       "- **Inpainting**: Filling in missing parts of images, such as restoring damaged photographs or completing artworks.\n",
       "- **Super Resolution**: Enhancing the resolution of images, which is particularly useful in medical imaging and satellite imagery.\n",
       "\n",
       "*Contextual Example*: A recent project utilized stable diffusion to create artwork that blends styles from different artists, showcasing the model's versatility in creative fields.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Stable diffusion models represent a powerful approach in the field of generative AI, leveraging the principles of diffusion processes to create high-quality data. By understanding the forward and reverse processes, as well as the underlying mathematical formulations, non-AI researchers can appreciate the sophistication and potential of these models. As the field continues to evolve, stable diffusion is likely to play a crucial role in various applications across different domains. Future developments may include improvements in model efficiency and new applications in areas such as virtual reality and personalized content creation.\n",
       "\n",
       "## References\n",
       "\n",
       "1. Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. *arXiv preprint arXiv:2006.11239*.\n",
       "2. Song, Y., & Ermon, S. (2019). Generative Modeling by Estimating Gradients of the Data Distribution. *arXiv preprint arXiv:1907.05600*.\n",
       "3. Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *arXiv preprint arXiv:2105.05233*."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_WRITER_SYS_MSG = \"\"\"\n",
    "You take in a research report and a set of bullet points with feedback on improve,\n",
    "and you revise the research report based on the feedback and write a final version\n",
    "\"\"\"\n",
    "\n",
    "prompt_final_writer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', FINAL_WRITER_SYS_MSG),\n",
    "        ('human','Write a Reviewed version of the report:\\n\\n {report},based on the following {feedback}\\n\\n')\n",
    "    ]\n",
    ")\n",
    "llm_final_writer = ChatOpenAI(model='gpt-4o-mini', temperature=0.2)\n",
    "\n",
    "chain_final_writer = prompt_final_writer | llm_final_writer | output_parser\n",
    "\n",
    "output_final_report =chain_final_writer.invoke({'report':output,'feedback':feedback_output})\n",
    "Markdown(output_final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
